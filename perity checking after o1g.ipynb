{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "# Filter out all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df=pd.DataFrame()\n",
    "# Use pd.concat instead of append\n",
    "df = pd.concat([df, pd.DataFrame([{\"1\": 1, \"2\": 2, \"3\": 3}])], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\RERA op\\rera main code\\o1b_20241231_rera_final_review_Pune.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # Create a blank DataFrame with the \"RERA ID\" column\n",
    "    blank_df = pd.DataFrame(columns=[\"RERA ID\"])\n",
    "    blank_df.to_csv(file_path, index=False)\n",
    "    print(f\"File did not exist. A blank file with 'RERA ID' column has been created at: {file_path}\")\n",
    "\n",
    "# Load the file\"\n",
    "df_rera_unique = pd.read_csv(file_path)\n",
    "print(df_rera_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from time import sleep\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementNotInteractableException, NoSuchElementException, TimeoutException\n",
    "import time\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import pytesseract\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a61e27",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def project_detail_page(url):\n",
    "#     webpage = requests.get(href,10)\n",
    "#     fsoup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "#     return fsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def project_detail_page(url):\n",
    "    # Set up the web driver\n",
    "    option = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options = option)\n",
    "\n",
    "    # Open the web page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to fully load (you can modify the condition as needed)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    fsoup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    return fsoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "def extract_dates_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts specific date patterns from a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted dates in the format DD/MM/YYYY.\n",
    "    \"\"\"\n",
    "    date = []  # List to store extracted dates\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:  # Ensure the page has text\n",
    "                    # Normalize the text by replacing multiple spaces with a single space\n",
    "                    normalized_text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "                    # Find dates with specific patterns\n",
    "                    dat = re.findall(r\"Dated: (\\d{2}/\\d{2}/\\d{4})\", normalized_text)\n",
    "                    date.extend(dat)\n",
    "                    \n",
    "                    cfrom = re.findall(r\"commencing from (\\d{2}/\\d{2}/\\d{4})\", normalized_text)\n",
    "                    date.extend(cfrom)\n",
    "                    \n",
    "                    end = re.findall(r\"ending with (\\d{2}/\\d{2}/\\d{4})\", normalized_text)\n",
    "                    date.extend(end)\n",
    "\n",
    "        return date if date else None  # Return dates or None if no matches found\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rera_data(fsoup):\n",
    "    df_past = pd.DataFrame(columns = [\"Organization\",\"Owners/Patrners\",\"Contact\",\"Project\",\"Project Name\",\"Type of Project\",\n",
    "                                    \"Others\",\"Land Area (in sqmtrs)\",\"Address\",\"CTS number\",\"Number of Buildings/Plot\",\n",
    "                                    \"Number Apartments\",\"Original Proposed Date of Completion\",\"Actual Date of Completion\",\n",
    "                                    \"website of Organization\"])\n",
    "    # go to where project details table is\n",
    "    content = fsoup.find('div',id = \"DivProject\").find_all('div',class_=\"x_content label-block\")\n",
    "\n",
    "    # details Litigations related to the project\n",
    "    part1 = content[0].find_all('div',class_=\"row\")\n",
    "\n",
    "    # FSI Details \"Sanctioned FSI of the project applied for registration\"\n",
    "    part2 = content[2].find_all('div',class_='row')\n",
    "\n",
    "\n",
    "    # Bank Details \"Bank Name\"\n",
    "    part3 = content[3].find_all('div',class_=\"row\")\n",
    "\n",
    "\n",
    "    # ******project details\n",
    "    temp = part1[0].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "\n",
    "\n",
    "    # project name\n",
    "    name=\"\"\n",
    "    try:\n",
    "        name = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "\n",
    "        name=\"NA\"\n",
    "    print(name)\n",
    "    status =\"\"\n",
    "    try:\n",
    "        # status of project\n",
    "        status = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        status=\"NA\"\n",
    "\n",
    "\n",
    "\n",
    "    # ******dates Related to projects\n",
    "    temp = part1[1].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    date=\"\"\n",
    "    try:\n",
    "        # Proposed Date of Completion\n",
    "        date = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        date=\"NA\"\n",
    "\n",
    "\n",
    "    rdate=\"\"\n",
    "    try:\n",
    "        # Revised Proposed Date of Completion\n",
    "        rdate = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        rdate=\"NA\"\n",
    "\n",
    "\n",
    "    # ************* Details of Litigations related to the project\n",
    "    temp = part1[3].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Type of the Project\n",
    "    type=\"\"\n",
    "    try:\n",
    "        type = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        type=\"NA\"\n",
    "\n",
    "\n",
    "    # Litigation Detail\n",
    "    litigations=\"\"\n",
    "    try:\n",
    "        litigations = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        litigations=\"NA\"\n",
    "\n",
    "\n",
    "    # *************** Details of Promoter (land owner or Investor)\n",
    "\n",
    "    temp = part1[4].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "#     Promoter (land owner or Investor)\n",
    "    promoter=\"\"\n",
    "    try:\n",
    "        promoter = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        promoter=\"NA\"\n",
    "\n",
    "\n",
    "    # ****************\n",
    "    temp = part1[6].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Plot Bearing No / CTS no  / Survey Number/Final Plot no.\n",
    "    no=\"\"\n",
    "    try:\n",
    "        no = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        no=\"NA\"\n",
    "\n",
    "\n",
    "    # Boundaries East\n",
    "    try:\n",
    "        east = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        east=\"NA\"\n",
    "\n",
    "\n",
    "    # ************************\n",
    "    temp = part1[7].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Boundaries West\n",
    "    try:\n",
    "        west = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        west=\"NA\"\n",
    "\n",
    "\n",
    "    # Boundaries North\n",
    "    try:\n",
    "        north = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        north=\"NA\"\n",
    "\n",
    "\n",
    "    # *********************\n",
    "    temp = part1[8].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "    try:\n",
    "        south = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        south=\"NA\"\n",
    "#         print(\"South\",south)\n",
    "\n",
    "    ####### same as North\n",
    "    try:\n",
    "        state = temp[3].text.strip()\n",
    "#         print('State', state)\n",
    "    except:\n",
    "        state=\"NA\"\n",
    "\n",
    "\n",
    "    # *********************\n",
    "    temp = part1[9].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # division\n",
    "    try:\n",
    "        division = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        division=\"NA\"\n",
    "\n",
    "\n",
    "    # District\n",
    "    try:\n",
    "        district = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        district=\"NA\"\n",
    "\n",
    "\n",
    "    # ************************\n",
    "    temp = part1[10].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # taluka\n",
    "    try:\n",
    "        taluka = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        taluka=\"NA\"\n",
    "\n",
    "\n",
    "    # Village\n",
    "    try:\n",
    "        village = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        village=\"NA\"\n",
    "\n",
    "\n",
    "    # *******************\n",
    "    temp = part1[11].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Street\n",
    "    try:\n",
    "        street = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        street=\"NA\"\n",
    "\n",
    "\n",
    "    # locality\n",
    "    try:\n",
    "        locality = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        locality=\"NA\"\n",
    "\n",
    "\n",
    "    # ***************\n",
    "    temp = part1[12].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # pincode\n",
    "    try:\n",
    "        pincode = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        pincode=\"NA\"\n",
    "\n",
    "\n",
    "    # area\n",
    "    try:\n",
    "        area = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        area=\"NA\"\n",
    "\n",
    "    # #Total Plot/Project area (sqmts)\n",
    "    # try:\n",
    "    #     label = fsoup.find('label', text='Total Plot/Project area (sqmts)')\n",
    "    #     value_div = label.find_parent('div').find_next_sibling('div')\n",
    "\n",
    "    #     # Print the extracted value\n",
    "    #     print(value_div.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ******************\n",
    "    temp = part1[13].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Total Number of Proposed Building/Wings (In the Layout/Plot)\n",
    "    try:\n",
    "        total_building = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        total_building=\"NA\"\n",
    "\n",
    "\n",
    "    # **********************\n",
    "    temp = part1[14].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Number of Sanctioned Building\n",
    "    try:\n",
    "        sanc_count = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        sanc_count=\"NA\"\n",
    "\n",
    "\n",
    "    # Proposed But Not Sanctioned Buildings Count\n",
    "    try:\n",
    "        notsanc_count = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        notsanc_count=\"NA\"\n",
    "\n",
    "\n",
    "    # ******************************\n",
    "    temp = part1[15].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # Total Recreational Open Space as Per Sanctioned Plan\n",
    "    # Agg Area\n",
    "    try:\n",
    "        agg_area=temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        agg_area=\"NA\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##-------********** Part 2\n",
    "\n",
    "    temp = part2[0].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # FSI Area\n",
    "    try:\n",
    "        FSI_area = temp[1].text.strip()\n",
    "#         print(\"FSI Area\",FSI_area)\n",
    "    except:\n",
    "        FSI_area=\"NA\"\n",
    "#         print(\"FSI Area\",FSI_area)\n",
    "\n",
    "    # FSI_area_approved\n",
    "    try:\n",
    "        FSI_area_approved = temp[3].text.strip()\n",
    "\n",
    "    except:\n",
    "        FSI_area_approved=\"NA\"\n",
    "\n",
    "\n",
    "    # ******************************\n",
    "    temp = part2[1].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # total_fsi\n",
    "    try:\n",
    "        total_fsi = temp[1].text.strip()\n",
    "\n",
    "    except:\n",
    "        total_fsi=\"NA\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---------*****************part 3\n",
    "    temp = part3[0].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # bank name\n",
    "    try:\n",
    "        Bank_name = temp[1].text.strip()\n",
    "    except:\n",
    "        Bank_name=\"NA\"\n",
    "\n",
    "    # *******************\n",
    "    temp = part3[1].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "    # IFSC Code\n",
    "    try:\n",
    "        ifsc = temp[1].text.strip()\n",
    "    except:\n",
    "        ifsc=\"NA\"\n",
    "\n",
    "#     @@@@@\n",
    "    # Address\n",
    "    table_present=\"No\"\n",
    "    add = \"\";\n",
    "    if(fsoup.find('div',id = \"fldFirm\")!= None):\n",
    "\n",
    "        # Organization - Name, Contact,\n",
    "        details = fsoup.find('div',id = \"fldFirm\").find_all('div',class_ = 'row')\n",
    "\n",
    "        # *************** Organization\n",
    "        temp = details[0].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "        # organization\n",
    "        try:\n",
    "            organization = temp[1].text.strip()\n",
    "        except:\n",
    "            organization=\"NA\"\n",
    "        # if len(df_rera_unique[(df_rera_unique['Project Name'].str.lower().str.strip() == name.lower().strip()) & (df_rera_unique['Organization/Individual'].str.lower().str.strip() == organization.lower().strip())]) > 0:\n",
    "        #     return 'Present'\n",
    "\n",
    "        # ******************** Organization Contact Details - Contact Number\n",
    "        temp = details[-2].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "        # contact\n",
    "        try:\n",
    "            contact = temp[1].text.strip()\n",
    "            if(contact==\"\"):\n",
    "                contact= \"NA\"\n",
    "        except:\n",
    "            contact=\"NA\"\n",
    "\n",
    "        # ************************** Organization Contact Details - Website URl\n",
    "        temp = details[-1].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "        # Website url\n",
    "        try:\n",
    "            website  = temp[1].text.strip()\n",
    "            if(website == \"\"):\n",
    "                website = \"NA\"\n",
    "        except:\n",
    "            website=\"NA\"\n",
    "\n",
    "\n",
    "        # Developer Details\n",
    "        # Address Details\n",
    "        developer_address = \"\"\n",
    "\n",
    "        temp1 = details[-8].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "        try:\n",
    "            # Block Number\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # Building Name\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # **********************\n",
    "            temp1 = details[-7].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "            # Street Name\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # Locality\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # **********************\n",
    "            temp1 = details[-6].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "            # Land mark\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # State/UT\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "\n",
    "            #\n",
    "            temp1 = details[-5].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "            # Division\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # District\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # *********************************\n",
    "            temp1 = details[-4].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "            # Taluka\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # Village\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            developer_address+=\",\"\n",
    "\n",
    "            # ********************\n",
    "            temp1 = details[-3].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "            # Pin Code\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "        except:\n",
    "            developer_address=\"NA\"\n",
    "\n",
    "\n",
    "    # if table is another table element\n",
    "    else:\n",
    "\n",
    "        details = fsoup.find('div',id = \"fldind\").find_all('div',class_ = 'row')\n",
    "        temp = details[0].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "\n",
    "        # organization name\n",
    "        try:\n",
    "            organization = temp[1].text.strip()+\"(Individual)\"\n",
    "        except:\n",
    "            organization = \"NA\"\n",
    "        # if len(df_rera_unique[(df_rera_unique['Project Name'].str.lower().str.strip() == name.lower().strip()) & (df_rera_unique['Organization/Individual'].str.lower().str.strip() == organization.lower().strip())]) > 0:\n",
    "        #     return 'Present'\n",
    "\n",
    "        # ******************** Organization Contact Details - Contact Number\n",
    "        temp = details[-3].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "        try:\n",
    "            contact = temp[1].text.strip()\n",
    "            if(contact==\"\"):\n",
    "                contact= \"NA\"\n",
    "        except:\n",
    "            contact = \"NA\"\n",
    "\n",
    "        # ******************** Organization Contact Details - Website url\n",
    "        temp = details[-1].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "        try:\n",
    "            website  = temp[1].text.strip()\n",
    "            if(website == \"\"):\n",
    "                website = \"NA\"\n",
    "        except:\n",
    "            website=\"NA\"\n",
    "        developer_address = \" \"\n",
    "\n",
    "        try:\n",
    "            temp1 = details[-9].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            temp1 = details[-8].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            temp1 = details[-7].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            temp1 = details[-6].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            temp1 = details[-5].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[3].text.strip()\n",
    "            temp1 = details[-4].find_all('div',class_=\"col-md-3 col-sm-3\")\n",
    "            developer_address+=\",\"\n",
    "            developer_address+=temp1[1].text.strip()\n",
    "        except Exception as e:\n",
    "            developer_address = \"NA\"\n",
    "#             print(\"developer_address\", developer_address)\n",
    "#     if len(df_rera_unique[(df_rera_unique['Project Name'].str.lower().str.strip() == name.lower().strip()) & (df_rera_unique['Organization/Individual'].str.lower().str.strip() == organization.lower().strip())]) > 0:\n",
    "#         return 'Present'\n",
    "\n",
    "#     print(developer_address)\n",
    "    # Member Information table\n",
    "    past_properties_data = []\n",
    "    past_name = \"\"\n",
    "    past_project = \"\"\n",
    "    past_type = \"\"\n",
    "    past_area = \"\"\n",
    "    past_address = \"\"\n",
    "    others = \"\"\n",
    "    cts_number = \"\"\n",
    "    no_past_ap = \"\"\n",
    "    no_past_plot = \"\"\n",
    "    past_date = \"\"\n",
    "    past_actual = \"\"\n",
    "    oname = \"\"\n",
    "\n",
    "\n",
    "    # Member Information table\n",
    "    oname =\"\"\n",
    "    try:\n",
    "        tabl = fsoup.find('div', id = 'DivMember' ).find('table',class_ = \"table\")\n",
    "        try:\n",
    "            members = tabl.find_all('tr')\n",
    "\n",
    "            # name of all members\n",
    "            for j in range(1,len(members)):\n",
    "                oname = oname+\",\"+members[j].find('td').text.strip()\n",
    "        except:\n",
    "            oname=\"NA\"\n",
    "    except:\n",
    "        oname=\"NA\"\n",
    "\n",
    "    # Past Experience Details\n",
    "    Past_experience_details = {}\n",
    "\n",
    "    # Past Experience table\n",
    "    try:\n",
    "        table = fsoup.find('table', class_='table table-striped grid-table')\n",
    "        try:\n",
    "\n",
    "                    # rows in the table\n",
    "            rows = table.find_all('tr', class_=\"grid-row\")\n",
    "            table_present = \"Yes\"\n",
    "            # all rows in a Table\n",
    "            for row in rows:\n",
    "                typ = row.find_all('td', class_=\"grid-cell\")\n",
    "\n",
    "                # getting Column content for ec=very row\n",
    "                for t in typ:\n",
    "                    a = t.get('data-name')\n",
    "                    # project Name\n",
    "                    if a == \"ProjectName\":\n",
    "                        past_name = t.text.strip()\n",
    "\n",
    "                    # Type of Project\n",
    "                    elif a == \"TypeofProject\":\n",
    "                        past_type = t.text\n",
    "                        if \"Residential\" in past_type:\n",
    "                            past_type=\"Residential\"\n",
    "                        elif \"Others\" in past_type:\n",
    "                            past_type = \"Others\"\n",
    "                        else:\n",
    "                            past_type = \"NA\"\n",
    "\n",
    "                    # others\n",
    "                    elif a == \"PTypeOther\":\n",
    "                        others = t.text.strip()\n",
    "                        if(others == \"\"):\n",
    "                            others = \"NA\"\n",
    "\n",
    "                    # Land Area\n",
    "                    elif a == \"LandArea\":\n",
    "                        past_area = t.text.strip()\n",
    "                        if(past_area == \"\"):\n",
    "                            past_area = \"NA\"\n",
    "                        else:\n",
    "                            try:\n",
    "                                past_area-re.sub(r'\\.{2,}', '.', past_area.split(\"+\")[0])\n",
    "                                past_area = float(past_area)\n",
    "                            except:\n",
    "                                past_area = past_area\n",
    "\n",
    "                    # past project address\n",
    "                    elif a == \"Address\":\n",
    "                        past_address = t.text.strip()\n",
    "                        if(past_address == \"\"):\n",
    "                            past_address = \"NA\"\n",
    "\n",
    "                    # CTS No\n",
    "                    elif a == \"CTSNo\":\n",
    "                        cts_number = t.text.strip()\n",
    "                        if(cts_number == \"\"):\n",
    "                            cts_number = \"NA\"\n",
    "#                         print(\"CTS No.\", cts_number)\n",
    "\n",
    "                    # Number of Buildings/Plot\n",
    "                    elif a == \"BuildPlotCount\":\n",
    "                        no_past_plot = t.text.strip()\n",
    "                        if(no_past_plot == \"\"):\n",
    "                            no_past_plot = \"NA\"\n",
    "\n",
    "                        else:\n",
    "                            no_past_plot = int(no_past_plot)\n",
    "\n",
    "                    # Number of Apartments\n",
    "                    elif a == \"Apartmentcount\":\n",
    "                        no_past_ap = t.text.strip()\n",
    "                        if(no_past_ap == \"\"):\n",
    "                            no_past_ap = \"NA\"\n",
    "                        else:\n",
    "                            no_past_ap = int(no_past_ap)\n",
    "#                         print(\"Number of Apartments\",no_past_ap)\n",
    "\n",
    "                    # Original Proposed Date of Completion\n",
    "                    elif a == \"ProjectStartDate\":\n",
    "                        past_date = t.text.strip()\n",
    "                        if(past_date == \"\"):\n",
    "                            past_date = \"NA\"\n",
    "\n",
    "                    # Actual Date of Completion\n",
    "                    elif a == \"ProjectEndDate\":\n",
    "                        past_actual = t.text.strip()\n",
    "                        if(past_actual == \"\"):\n",
    "                            past_actual = \"NA\"\n",
    "#                         print(\"Actual Date of Completion\",past_actual)\n",
    "\n",
    "                    past_project = {\n",
    "                        \"Organization\": organization,\n",
    "                        \"Owners/Patrners\" : oname,\n",
    "                        \"Contact\":contact,\n",
    "                        \"Project\": name,\n",
    "                        \"Project Name\": past_name,\n",
    "                        \"Type of Project\": past_type,\n",
    "                        \"Others\": others,\n",
    "                        \"Land Area (in sqmtrs)\": past_area,\n",
    "                        \"Address\": past_address,\n",
    "                        \"CTS number\": cts_number,\n",
    "                        \"Number of Buildings/Plot\": no_past_plot,\n",
    "                        \"Number Apartments\": no_past_ap,\n",
    "                        \"Original Proposed Date of Completion\": past_date,\n",
    "                        \"Actual Date of Completion\": past_actual,\n",
    "                        \"website of Organization\":website\n",
    "                    }\n",
    "                    details = {'Contact':contact,\n",
    "                                'Type of Project': past_type,\n",
    "                                'Others': others,\n",
    "                                'Land Area(Sq mtrs)': past_area,\n",
    "                                'Address': past_address,\n",
    "                                'CTS-Number': cts_number,\n",
    "                                'Number of Buildings/plot': no_past_plot,\n",
    "                                'Number of Apartmennts': no_past_ap,\n",
    "                                'Original Proposed Date of Completion': past_date,\n",
    "                                'Actual Date of Completion': past_actual,\n",
    "                                'website of organization': website}\n",
    "                    Past_experience_details[past_name] = details\n",
    "                    df_past = df_past.append(past_project,ignore_index=True)\n",
    "\n",
    "\n",
    "        except:\n",
    "#             print(\"error in past expreience1\")\n",
    "            None\n",
    "\n",
    "\n",
    "    except:\n",
    "#         print(\"error in past expreience2\")\n",
    "        None\n",
    "\n",
    "    df = pd.DataFrame(columns = [\"Project Name\",\"Project Status\",\"Organization/Individual\",\"developer_address\",\"Proposed Date of Completion\",\"Revised Proposed Date of Completion\",\n",
    "        \"Litigations related to the project ?\",\"Project type\",\"Plot no.\",\"Building_name\",\"Number of Sanctioned Floors\",\n",
    "        \"Number of Basement's\",\"Number of Plinth\",\"Number of Podium's\",\"Number of Stilts\",\"Total_no_of_open_Parking\",\"Number_of_Closed_Parking\",\"boundaries east\",\n",
    "        \"boundaries west\",\"boundaries north\",\"boundaries south\",\"state\",\"division\",\"district\",\"taluka\",\"village\",\"street\",\"locality\",\n",
    "        \"pin code\",\"Total Building Count\",\"Sanctioned Buildings Count\",\"Proposed But Not Sanctioned Buildings Count\",\"Aggregate area(In sqmts) of recreational open space\",\n",
    "        \"Built-up-Area as per Proposed FSI (In sqmts) ( Proposed but not sanctioned)\",\"Built-up-Area as per Approved FSI (In sqmts)\",\n",
    "        \"TotalFSI\",\"bhks\",\"Bank_name\",\"IFSC CODE\",\"website of Organization\",\"Contact\",\"Past Experience\",\"Past_experience_details\"])\n",
    "\n",
    "\n",
    "    # Building Details\n",
    "    tabless = fsoup.find_all('table', class_=\"table table-bordered table-responsive table-striped\")\n",
    "    try:\n",
    "        build_name_list = []  # Initialize an empty list to store the output\n",
    "        for table in tabless:\n",
    "            rows = table.find_all('tr')  # Find all rows in the table\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) == 11:\n",
    "                    # Extracting data from the 3rd and 4th columns in the row\n",
    "                    key = columns[2].text.strip()\n",
    "                    value = columns[3].text.strip()\n",
    "\n",
    "                    # Replace empty strings with 'Not Available'\n",
    "                    value = 'Not Available' if not value else value\n",
    "\n",
    "                    # Append key-value pair as a list to the main list\n",
    "                    build_name_list.append([key, value])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Print or use the final list of lists\n",
    "    print(build_name_list)\n",
    "\n",
    "\n",
    "    # Building Details\n",
    "    # Lists to hold all building info for a single project\n",
    "    headers = [h.text.strip() for h in fsoup.find_all(\"h2\")]\n",
    "    building_names = []\n",
    "    NOBs = []\n",
    "    NOPs = []\n",
    "    NOPOs = []\n",
    "    NOSs = []\n",
    "    total_parking_lists = []\n",
    "    total_closed_parking_lists = []\n",
    "    total_floors_list = []\n",
    "    all_bhks = []\n",
    "    plot_details = \"NA\"\n",
    "    BHK_wise_CA = []\n",
    "    Carpet_wise_total_sold = []\n",
    "\n",
    "    # Building Details\n",
    "    if \"Building Details\" in headers:\n",
    "        building_details = fsoup.find('div', id='DivBuilding')\\\n",
    "            .find('table', class_=\"table table-bordered table-responsive table-striped\")\\\n",
    "            .find_all('tr')\n",
    "\n",
    "        for j in range(0, len(building_details)):\n",
    "            bhks = {}\n",
    "            cbhks = {}\n",
    "\n",
    "            try:\n",
    "                if building_details[0 + j].find_all('th')[0].text == 'Sr.No.':\n",
    "                    pass\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                one = building_details[0 + j].find_all('th')\n",
    "                two = building_details[1 + j].find_all('td')\n",
    "                three = building_details[2 + j].find_all('td')[2].find_all('tr')\n",
    "\n",
    "                # Building Name\n",
    "                try:\n",
    "                    if (one[2].text.strip() == \"Name (Also mention identification of building/wing/other as per approved plan)\"):\n",
    "                        building_names.append(two[2].text.strip())\n",
    "                    else:\n",
    "                        building_names.append(\"NA\")\n",
    "                except:\n",
    "                    building_names.append(\"NA\")\n",
    "\n",
    "                # Number of Basement's\n",
    "                try:\n",
    "                    if (one[4].text.strip() == \"Number of Basement's\"):\n",
    "                        NOBs.append(two[4].text.strip())\n",
    "                    else:\n",
    "                        NOBs.append(\"NA\")\n",
    "                except:\n",
    "                    NOBs.append(\"NA\")\n",
    "\n",
    "                # Number of Plinth\n",
    "                try:\n",
    "                    if (one[5].text.strip() == \"Number of Plinth\"):\n",
    "                        NOPs.append(two[5].text.strip())\n",
    "                    else:\n",
    "                        NOPs.append(\"NA\")\n",
    "                except:\n",
    "                    NOPs.append(\"NA\")\n",
    "\n",
    "                # Number of Podium's\n",
    "                try:\n",
    "                    if (one[6].text.strip() == \"Number of Podium's\"):\n",
    "                        NOPOs.append(two[6].text.strip())\n",
    "                    else:\n",
    "                        NOPOs.append(\"NA\")\n",
    "                except:\n",
    "                    NOPOs.append(\"NA\")\n",
    "\n",
    "                # Number of Sanctioned Floors\n",
    "                try:\n",
    "                    if (one[7].text.strip() == \"Number of Sanctioned Floors\"):\n",
    "                        total_floors_list.append(int(two[7].text.strip()))\n",
    "                    else:\n",
    "                        total_floors_list.append(0)\n",
    "                except:\n",
    "                    total_floors_list.append(0)\n",
    "\n",
    "                # Number of Stilts\n",
    "                try:\n",
    "                    if (one[8].text.strip() == \"Number of Stilts\"):\n",
    "                        NOSs.append(two[8].text.strip())\n",
    "                    else:\n",
    "                        NOSs.append(\"NA\")\n",
    "                except:\n",
    "                    NOSs.append(\"NA\")\n",
    "\n",
    "                # Total open parking\n",
    "                try:\n",
    "                    if (one[9].text.strip()):\n",
    "                        total_parking_lists.append(two[9].text.strip())\n",
    "                    else:\n",
    "                        total_parking_lists.append(\"NA\")\n",
    "                except:\n",
    "                    total_parking_lists.append(\"NA\")\n",
    "\n",
    "                # Total closed parking\n",
    "                try:\n",
    "                    if (one[10].text.strip()):\n",
    "                        total_closed_parking_lists.append(two[10].text.strip())\n",
    "                    else:\n",
    "                        total_closed_parking_lists.append(\"NA\")\n",
    "                except:\n",
    "                    total_closed_parking_lists.append(\"NA\")\n",
    "\n",
    "                # BHK details\n",
    "                bhks = {}\n",
    "                cbhks = {}\n",
    "                for row in range(1, len(three)):\n",
    "                    try:\n",
    "                        inner = three[row].find_all('td')\n",
    "                        bhk_type = inner[1].text.strip()\n",
    "\n",
    "                        if bhk_type in bhks:\n",
    "                            bhks[bhk_type]['total'] += int(inner[3].text)\n",
    "                            bhks[bhk_type]['sold'] += int(inner[4].text)\n",
    "                            bhks[bhk_type]['avg_area'] = (\n",
    "                                (bhks[bhk_type]['avg_area'] * cbhks[bhk_type]) +\n",
    "                                float(re.sub(r'\\.{2,}', '.', inner[2].text.split(\"+\")[0]))\n",
    "                            ) / (cbhks[bhk_type] + 1)\n",
    "                            cbhks[bhk_type] += 1\n",
    "                        else:\n",
    "                            cbhks[bhk_type] = 1\n",
    "                            bhks[bhk_type] = {\n",
    "                                'total': int(inner[3].text),\n",
    "                                'sold': int(inner[4].text),\n",
    "                                'avg_area': float(re.sub(r'\\.{2,}', '.', inner[2].text.split(\"+\")[0]))\n",
    "                            }\n",
    "                    except Exception as e:\n",
    "                        print(\"BHK Error:\", e)\n",
    "                        continue\n",
    "                all_bhks.append(bhks)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Outer loop error:\", e)\n",
    "                continue\n",
    "        #BHK wise CA\n",
    "    #     try:\n",
    "        try:\n",
    "            all_tables = fsoup.find_all('table', class_=\"table table-bordered table-responsive table-striped\")\n",
    "            bhk_tables = []\n",
    "\n",
    "            # Filter only BHK-wise tables based on presence of Apartment Type & Carpet Area headers\n",
    "            for table in all_tables:\n",
    "                headers = [th.text.strip().lower() for th in table.find_all('th')]\n",
    "                if 'apartment type' in headers and 'carpet area (in sqmts)' in headers:\n",
    "                    bhk_tables.append(table)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] While filtering BHK tables: {e}\")\n",
    "\n",
    "        # --- BHK-wise Carpet Area ---\n",
    "        try:\n",
    "            \n",
    "            for table in bhk_tables:\n",
    "                wing_dict = {}\n",
    "                rows = table.find_all('tr')[1:]  # Skip header\n",
    "                for row in rows:\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) == 5:\n",
    "                        apt_type = cols[1].text.strip().lower()\n",
    "                        carpet_area = cols[2].text.strip()\n",
    "                        wing_dict.setdefault(apt_type, []).append(carpet_area)\n",
    "                if wing_dict:\n",
    "                    BHK_wise_CA.append(wing_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] While extracting BHK-wise carpet area: {e}\")\n",
    "\n",
    "        # --- Carpet-wise Total vs Sold Apartments ---\n",
    "        try:\n",
    "            for table in bhk_tables:\n",
    "                wing_dict = {}\n",
    "                rows = table.find_all('tr')[1:]  # Skip header\n",
    "                for row in rows:\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) == 5:\n",
    "                        apt_type = cols[1].text.strip().lower()\n",
    "                        carpet_area = cols[2].text.strip()\n",
    "                        total_apts = int(cols[3].text.strip())\n",
    "                        booked_apts = int(cols[4].text.strip())\n",
    "\n",
    "                        if apt_type not in wing_dict:\n",
    "                            wing_dict[apt_type] = {}\n",
    "\n",
    "                        if carpet_area not in wing_dict[apt_type]:\n",
    "                            wing_dict[apt_type][carpet_area] = [0, 0]\n",
    "\n",
    "                        wing_dict[apt_type][carpet_area][0] += total_apts\n",
    "                        wing_dict[apt_type][carpet_area][1] += booked_apts\n",
    "\n",
    "                if wing_dict:\n",
    "                    Carpet_wise_total_sold.append(wing_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] While extracting carpet-wise total/booked data: {e}\")\n",
    "        \n",
    "    if \"Plot Details\" in headers:\n",
    "        try:\n",
    "            all_tables = fsoup.find_all(\"table\", class_=\"table table-bordered table-responsive table-striped\")\n",
    "            plot_table = None\n",
    "            for table in all_tables:\n",
    "                headers_row = table.find('tr')\n",
    "                if headers_row and any(\"Plot\" in th.text for th in headers_row.find_all('th')):\n",
    "                    plot_table = table\n",
    "                    break\n",
    "            if plot_table:\n",
    "                plot_details = [[cell.text.strip() for cell in row.find_all('td')] for row in plot_table.find_all('tr')[1:]]\n",
    "            else:\n",
    "                plot_details = \"NA\"\n",
    "        except (AttributeError, IndexError) as e:\n",
    "            print(f\"Error parsing Plot Details: {e}\")\n",
    "            plot_details = \"NA\"\n",
    "\n",
    "\n",
    "\n",
    "    project={\n",
    "    # \"rera_id\":ids[i],\n",
    "        \"Project Name\":name,\n",
    "        \"Project Status\":status,\n",
    "        # \"latitude\":lat[i],\n",
    "        # \"longitude\":lng[i],\n",
    "        \"Organization/Individual\":organization,\n",
    "        \"developer_address\":developer_address,\n",
    "        \"Proposed Date of Completion\":date,\n",
    "        \"Revised Proposed Date of Completion\":rdate,\n",
    "        \"Litigations related to the project ?\":litigations,\n",
    "        \"Project type\": type,\n",
    "        \"Plot no.\":no,\n",
    "        \"Building_name\":building_names,\n",
    "        \"Number of Sanctioned Floors\":total_floors_list,\n",
    "        \"Number of Basement's\":NOBs,\n",
    "        \"Number of Plinth\":NOPs,\n",
    "        \"Number of Podium's\":NOPOs,\n",
    "        \"Number of Stilts\":NOSs,\n",
    "        \"Total_no_of_open_Parking\":total_parking_lists,\n",
    "        \"Number_of_Closed_Parking\":total_closed_parking_lists,\n",
    "        \"boundaries east\":east,\n",
    "        \"boundaries west\":west,\n",
    "        \"boundaries north\":north,\n",
    "        \"boundaries south\":south,\n",
    "        \"state\":state,\n",
    "        \"division\":division,\n",
    "        \"district\":district,\n",
    "        \"taluka\":taluka,\n",
    "        \"village\":village,\n",
    "        \"street\":street,\n",
    "        \"locality\":locality,\n",
    "        \"pin code\":pincode,\n",
    "        \"Total Plot/Project area (sqmts)\":area,\n",
    "        \"Total Building Count\":total_building,\n",
    "        \"Sanctioned Buildings Count\":sanc_count,\n",
    "        \"Proposed But Not Sanctioned Buildings Count\":notsanc_count,\n",
    "        \"Aggregate area(In sqmts) of recreational open space\":agg_area,\n",
    "        \"Sactioned FSI of the Project Applied for Registration (Sanctioned Build-up Area)\":FSI_area,\n",
    "        \"Built-up-Area as per Proposed FSI (in sqfts)(Proposed but not Sanctioned)\":FSI_area_approved,\n",
    "        \"Permissible Built-up Area\":total_fsi,\n",
    "        \"bhks\":all_bhks,\n",
    "        \"Bank_name\":Bank_name,\n",
    "        \"IFSC CODE\":ifsc,\n",
    "        \"website of Organization\":website,\n",
    "        \"Contact\":contact,\n",
    "        \"Past Experience\":table_present,\n",
    "        \"Past_experience_details\":Past_experience_details,\n",
    "        \"plot_details_in_No_of_Plot/Area_of_plot/Booked_plot\": plot_details,\n",
    "        \"Building_wise_completion_date\":build_name_list,\n",
    "        \"BHK_wise_CA\":BHK_wise_CA,\n",
    "        \"Carpet_wise_total_sold\":Carpet_wise_total_sold\n",
    "    }\n",
    "    # df=pd.DataFrame(project)\n",
    "    if isinstance(project, dict):\n",
    "        project = pd.DataFrame([project])\n",
    "    \n",
    "    # Use pd.concat() to append the new row(s)\n",
    "    df = pd.concat([df, project], ignore_index=True)\n",
    "\n",
    "#         print(building_data_list)\n",
    "\n",
    "    return df_past,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c19cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options = option)\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"Enhance the image for better OCR accuracy and return the OCR result.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('L')  # Convert to grayscale\n",
    "    image = image.filter(ImageFilter.MedianFilter())  # Apply median filter\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(2)  # Increase contrast\n",
    "\n",
    "    # Save the enhanced image\n",
    "    enhanced_image_path = 'captcha_enhanced.png'\n",
    "    image.save(enhanced_image_path)\n",
    "\n",
    "    # Use OCR to read captcha\n",
    "    captcha_text = pytesseract.image_to_string(image).strip()\n",
    "    return captcha_text\n",
    "\n",
    "def clean_captcha(captcha_text):\n",
    "    \"\"\"Remove unwanted characters from the OCR result.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', captcha_text)\n",
    "\n",
    "try:\n",
    "    # Load the website\n",
    "    link = \"https://maharerait.mahaonline.gov.in/searchlist/search?MenuID=1069\"\n",
    "    driver.get(link)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(2)\n",
    "    # Click on registration dropdown\n",
    "    a = driver.find_element(By.ID, 'Promoter')\n",
    "    a.click()\n",
    "    # # Click on Register real estate agent\n",
    "    # b = driver.find_element(By.XPATH, \"/html/body/header/div[5]/div/div/div/ul/li[5]/ul/li[1]/a\")\n",
    "    # b.click()\n",
    "    # # Handle pop-up alert\n",
    "    # driver.switch_to.alert.accept()\n",
    "    # driver.switch_to.window(driver.window_handles[1])\n",
    "    # time.sleep(5)\n",
    "    # Click on advance search\n",
    "    advance_search = driver.find_element(By.XPATH, \"/html/body/section/div/div/form/div[2]/div[2]/div[8]/div/div/div[1]/input\")\n",
    "    advance_search.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Select user type\n",
    "    bubble = driver.find_element(By.XPATH, \"/html/body/section/div/div/form/div[2]/div[2]/div[1]/div/div[2]/input[1]\")\n",
    "    bubble.click()\n",
    "\n",
    "    # Select state\n",
    "    state = driver.find_element(By.XPATH,\"/html/body/section/div/div/form/div[2]/div[2]/div[4]/div/div[2]/select/option[4]\")\n",
    "    state.click()\n",
    "    time.sleep(2)\n",
    "    # Select district\n",
    "    district = driver.find_element(By.XPATH,\"/html/body/section/div/div/form/div[2]/div[2]/div[5]/div[1]/div[2]/select/option[27]\")\n",
    "    district.click()\n",
    "\n",
    "    while True:\n",
    "        # Wait for the captcha image to load\n",
    "        captcha_image_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, 'CaptchaImage'))\n",
    "        )\n",
    "\n",
    "        # Screenshot the captcha image\n",
    "        captcha_image_path = 'captcha_screenshot.png'\n",
    "        captcha_image_element.screenshot(captcha_image_path)\n",
    "\n",
    "        # Attempt to read the CAPTCHA using OCR\n",
    "        captcha_text = process_image(captcha_image_path)\n",
    "        cleaned_captcha_text = clean_captcha(captcha_text)\n",
    "        print(f\"Raw OCR Captcha text: {captcha_text}\")\n",
    "        print(f\"Cleaned Captcha text: {cleaned_captcha_text}\")\n",
    "\n",
    "        # Enter captcha\n",
    "        try:\n",
    "            captcha_input = driver.find_element(By.ID, 'CaptchaInputText')\n",
    "            captcha_input.clear()\n",
    "            captcha_input.send_keys(cleaned_captcha_text)\n",
    "            print(cleaned_captcha_text)\n",
    "        except (ElementNotInteractableException, NoSuchElementException) as e:\n",
    "            print(f\"Error entering CAPTCHA text: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Click on search\n",
    "        try:\n",
    "            submit = driver.find_element(By.ID, \"btnSearch\")\n",
    "            submit.click()\n",
    "        except (ElementNotInteractableException, NoSuchElementException) as e:\n",
    "            print(f\"Error clicking search button: {e}\")\n",
    "            continue\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        try:\n",
    "            # Check if the \"Captcha is not valid\" popup appears\n",
    "            error_popup = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'sweet-alert'))\n",
    "            )\n",
    "            if error_popup.is_displayed():\n",
    "                print(\"Invalid CAPTCHA, retrying...\")\n",
    "                ok_button = error_popup.find_element(By.CLASS_NAME, 'confirm')\n",
    "                ok_button.click()\n",
    "            else:\n",
    "                break\n",
    "        except TimeoutException:\n",
    "            # If no error popup, break the loop\n",
    "            break\n",
    "\n",
    "    # Continue with your further code...\n",
    "\n",
    "\n",
    "finally:\n",
    "    print(\"Captcha accepted\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4505bb",
   "metadata": {},
   "source": [
    "# settings to change in driver opened window\n",
    "1. open the autodownload option\n",
    "2. change the download directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b570062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "# Directory to store downloads\n",
    "download_directory = r\"D:\\rera code\"\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "rera_final = pd.DataFrame()\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "def wait_for_download(filepath, timeout=30):\n",
    "    \"\"\"Waits for the file to be completely downloaded.\"\"\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "            return True\n",
    "        sleep(1)\n",
    "    return False\n",
    "\n",
    "def process_project(result, i):\n",
    "    \"\"\"Function to scrape and process one project.\"\"\"\n",
    "    global rera_final  # To update the DataFrame across threads\n",
    "    \n",
    "    try:\n",
    "        # Extract RERA ID\n",
    "        name = result.find_elements(By.CSS_SELECTOR, '[title=\"Download Certificate\"]')[0].get_attribute('data-docname') + \".pdf\"\n",
    "        rera_id = name[:-4]\n",
    "        \n",
    "        # Skip existing RERA IDs\n",
    "        if not df_rera_unique[df_rera_unique['RERA ID'].str.lower().str.strip().eq(rera_id.lower().strip())].empty:\n",
    "            return f\"Skipping RERA ID {rera_id}, already exists.\"\n",
    "\n",
    "        # Extract Last Modified Date\n",
    "        last_modified_date = result.find_element(By.CSS_SELECTOR, 'td[data-name=\"lastModifiedDate\"]').text\n",
    "        Promoter_name = result.find_element(By.CSS_SELECTOR, 'td[data-name=\"Name\"]').text\n",
    "\n",
    "\n",
    "        # Download the certificate\n",
    "        result.find_elements(By.CSS_SELECTOR, '[title=\"Download Certificate\"]')[0].click()\n",
    "        filepath = os.path.join(download_directory, name)\n",
    "        \n",
    "        if not wait_for_download(filepath):\n",
    "            return f\"Download timed out for RERA ID {rera_id}. Skipping PDF processing.\"\n",
    "\n",
    "        # Extract dates from PDF\n",
    "        date = [\"NA\", \"NA\", \"NA\"]\n",
    "        try:\n",
    "            date = extract_dates_from_pdf(filepath)\n",
    "            os.remove(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting date from PDF for {rera_id}: {e}\")\n",
    "\n",
    "        # Extract project details\n",
    "        try:\n",
    "            href = result.find_elements(By.CLASS_NAME, \"grid-cell\")[4].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "            fsoup = project_detail_page(href)\n",
    "            df_past, df_final = rera_data(fsoup)\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting project details for {rera_id}: {e}\"\n",
    "\n",
    "        # Add additional columns\n",
    "        df_final['Rera_certificate_bottom_left_date'] = date[0]\n",
    "        df_final['Rera_commmencing_from'] = date[1]\n",
    "        df_final['Rera_commmencing_ending_with'] = date[2]\n",
    "        df_final['Promoter Name'] = Promoter_name\n",
    "        df_final['Last_Modified_Date'] = last_modified_date\n",
    "        df_final['RERA ID'] = rera_id\n",
    "\n",
    "        # Append to the final DataFrame using concat\n",
    "        rera_final = pd.concat([rera_final, df_final], ignore_index=True)\n",
    "\n",
    "        # Store data in CSV\n",
    "        past_csv_path = os.path.join(download_directory, \"o1a_20241231_Past_final_review_Pune.csv\")\n",
    "        final_csv_path = os.path.join(download_directory, \"o1b_20241231_rera_final_review_Pune.csv\")\n",
    "        \n",
    "        df_past.to_csv(past_csv_path, mode='a', index=False, header=not os.path.isfile(past_csv_path))\n",
    "        df_final.to_csv(final_csv_path, mode='a', index=False, header=not os.path.isfile(final_csv_path))\n",
    "\n",
    "        return f\"Successfully processed RERA ID {rera_id}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing project {i + 1}: {e}\"\n",
    "\n",
    "\n",
    "# Web Scraping Execution\n",
    "page_count = 1\n",
    "project_count = 0  # Counter for processed projects\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = driver.find_element(By.XPATH, \"/html/body/section/div/div/form/div[3]/div/div[2]/div[1]/div/table/tbody\")\n",
    "        results = result.find_elements(By.CLASS_NAME, \"grid-row\")\n",
    "    except Exception as e:\n",
    "        print(\"Error locating table rows:\", e)\n",
    "        break\n",
    "\n",
    "    if page_count > 0:\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:  # Limit max workers to 10 for better efficiency\n",
    "            future_to_project = {executor.submit(process_project, results[i], i): i for i in range(len(results))}\n",
    "            \n",
    "            for future in as_completed(future_to_project):\n",
    "                print(future.result())  # Print status of each project\n",
    "                project_count += 1  # Increment count of processed projects\n",
    "                \n",
    "                if project_count >= 50:  # Stop after processing 50 projects\n",
    "                    break\n",
    "\n",
    "        print(f\"Page Number {page_count} DONE\")\n",
    "\n",
    "    if project_count >= 50:\n",
    "        break  # Stop processing after 50 projects\n",
    "\n",
    "    # Move to the next page\n",
    "    page_count += 1\n",
    "    go_to_next_page = input(\"Enter any key to proceed to the next page (or 'q' to quit): \").strip().lower()\n",
    "    \n",
    "    if go_to_next_page == 'q':\n",
    "        break\n",
    "    \n",
    "    sleep(5)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Total time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal Time Taken to Scrape 50 Projects: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1b2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dd628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36471b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a138ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"D:\\Rera_Scraping_March_25\\o1b_20241231_rera_final_review_Thane.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e847b",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "###  **Final Execution: Parity Check Between BHK and Carpet Data (Before & After Fixes)**\n",
    "\n",
    "This cell:\n",
    "- Extracts total, sold, and count values from `bhks` and original `Carpet_wise_total_sold`\n",
    "- Saves the **original carpet and CA data** before processing\n",
    "- **Drops the 0th index** from both `Carpet_wise_total_sold` and `BHK_wise_CA` if list length > 1 (to remove duplicates)\n",
    "- Aligns `{}` entries in `Carpet_wise_total_sold` and `BHK_wise_CA` based on blank entries in `bhks`\n",
    "- Recalculates all carpet stats **after corrections**\n",
    "- Adds:\n",
    "  - `parity_before`  compares bhk with carpet before fix\n",
    "  - `parity_after`  compares bhk with carpet after fix\n",
    "- Outputs cleaned columns for manual cross-verification\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Demo_bhk_wise_CA.xlsx\")\n",
    "\n",
    "# Step 1: Parse JSON safely\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [] if isinstance(x, str) and x.strip().startswith(\"[\") else {}\n",
    "\n",
    "cols_to_parse = [\n",
    "    \"BHKs_Updated\",\n",
    "    \"Carpet_wise_total_sold_converted_avg_converted\",\n",
    "    \"BHK_wise_CA_Updated\"\n",
    "]\n",
    "\n",
    "for col in cols_to_parse:\n",
    "    df[col] = df[col].apply(safe_literal_eval)\n",
    "\n",
    "# Step 2: Extract initial totals\n",
    "def extract_all_counts(row):\n",
    "    bhk_total = bhk_sold = 0\n",
    "    bhk_data = row[\"BHKs_Updated\"]\n",
    "    if isinstance(bhk_data, list):\n",
    "        for b in bhk_data:\n",
    "            if isinstance(b, dict):\n",
    "                for unit in b.values():\n",
    "                    bhk_total += unit.get(\"total\", 0)\n",
    "                    bhk_sold += unit.get(\"sold\", 0)\n",
    "\n",
    "    carpet_total = carpet_sold = 0\n",
    "    carpet_data = row[\"Carpet_wise_total_sold_converted_avg_converted\"]\n",
    "    if isinstance(carpet_data, list):\n",
    "        for entry in carpet_data:\n",
    "            if isinstance(entry, dict):\n",
    "                for flat in entry.values():\n",
    "                    for count in flat.values():\n",
    "                        carpet_total += count[0]\n",
    "                        carpet_sold += count[1]\n",
    "\n",
    "    return pd.Series([\n",
    "        bhk_total, bhk_sold, len(bhk_data) if isinstance(bhk_data, list) else 0,\n",
    "        carpet_total, carpet_sold, len(carpet_data) if isinstance(carpet_data, list) else 0\n",
    "    ])\n",
    "\n",
    "df[[\n",
    "    \"bhk_total\", \"bhk_sold\", \"bhk_count\",\n",
    "    \"carpet_total_before\", \"carpet_sold_before\", \"carpet_count_before\"\n",
    "]] = df.apply(extract_all_counts, axis=1)\n",
    "\n",
    "# Step 3: Drop 0th index from carpet and CA (if needed)\n",
    "df[\"Carpet_wise_total_sold_converted_avg_converted\"] = df[\"Carpet_wise_total_sold_converted_avg_converted\"].apply(\n",
    "    lambda x: x[1:] if isinstance(x, list) and len(x) > 1 else x\n",
    ")\n",
    "df[\"BHK_wise_CA_Updated\"] = df[\"BHK_wise_CA_Updated\"].apply(\n",
    "    lambda x: x[1:] if isinstance(x, list) and len(x) > 1 else x\n",
    ")\n",
    "\n",
    "# Step 4: Fix alignment if bhk_count > carpet_count\n",
    "def fix_alignment(row):\n",
    "    bhk_len = len(row[\"BHKs_Updated\"]) if isinstance(row[\"BHKs_Updated\"], list) else 0\n",
    "    carpet = row[\"Carpet_wise_total_sold_converted_avg_converted\"]\n",
    "    ca = row[\"BHK_wise_CA_Updated\"]\n",
    "\n",
    "    if isinstance(carpet, list) and isinstance(ca, list):\n",
    "        while len(carpet) < bhk_len:\n",
    "            carpet.append({})\n",
    "        while len(ca) < bhk_len:\n",
    "            ca.append({})\n",
    "\n",
    "    row[\"Carpet_wise_total_sold_converted_avg_converted\"] = carpet\n",
    "    row[\"BHK_wise_CA_Updated\"] = ca\n",
    "    return row\n",
    "\n",
    "df = df.apply(fix_alignment, axis=1)\n",
    "\n",
    "# Step 5: Recompute carpet totals after alignment\n",
    "def extract_carpet_after(row):\n",
    "    total = sold = 0\n",
    "    data = row[\"Carpet_wise_total_sold_converted_avg_converted\"]\n",
    "    if isinstance(data, list):\n",
    "        for entry in data:\n",
    "            if isinstance(entry, dict):\n",
    "                for flat in entry.values():\n",
    "                    for count in flat.values():\n",
    "                        total += count[0]\n",
    "                        sold += count[1]\n",
    "    return pd.Series([total, sold, len(data)])\n",
    "\n",
    "df[[\"carpet_total_after\", \"carpet_sold_after\", \"carpet_count_after\"]] = df.apply(extract_carpet_after, axis=1)\n",
    "\n",
    "# Step 6: Parity checks\n",
    "df[\"parity_before\"] = (\n",
    "    (df[\"bhk_total\"] == df[\"carpet_total_before\"]) &\n",
    "    (df[\"bhk_sold\"] == df[\"carpet_sold_before\"]) &\n",
    "    (df[\"bhk_count\"] == df[\"carpet_count_before\"])\n",
    ")\n",
    "\n",
    "df[\"parity_after\"] = (\n",
    "    (df[\"bhk_total\"] == df[\"carpet_total_after\"]) &\n",
    "    (df[\"bhk_sold\"] == df[\"carpet_sold_after\"]) &\n",
    "    (df[\"bhk_count\"] == df[\"carpet_count_after\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd028a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\Rera_Scraping_March_25\\Demo_bhk_wise_CA_parity.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parity_after'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\RERA op\\rera main code\\o1d_berfore_manual.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel(r\"D:\\RERA op\\rera main code\\demoo.xlsx\",sheet_name='Sheet2')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbde7b7",
   "metadata": {},
   "source": [
    "#MANUAL PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c63b6",
   "metadata": {},
   "source": [
    "# SQFT_to_SQMT Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc80602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "#File and sheet info\n",
    "# file_path = r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_before_Processing.xlsx\"\n",
    "\n",
    "#Load CSV\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# # Clean column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# Target columns\n",
    "# column_name = \"Carpet_wise_total_sold\"\n",
    "# unit_column = \"cwts(sqm/sqft)\"\n",
    "\n",
    "# Helper to check if value is numeric\n",
    "def is_numeric_string(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Main conversion logic with merge logic for duplicate sqm keys\n",
    "def convert_sqft_to_sqmt(data_str):\n",
    "    try:\n",
    "        data = ast.literal_eval(data_str)\n",
    "    except:\n",
    "        return data_str  # Return as-is if not parseable\n",
    "\n",
    "    # Check for \"3 bhk\"\n",
    "    has_3bhk = \"3bhk\" in data_str.lower()\n",
    "    min_sqft = 150 if has_3bhk else 100\n",
    "\n",
    "    def convert_keys(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, dict):\n",
    "                    new_inner = {}\n",
    "                    for inner_k, inner_v in v.items():\n",
    "                        if is_numeric_string(inner_k) and float(inner_k) >= min_sqft:\n",
    "                            sqm = round(float(inner_k) * 0.092903, 2)\n",
    "                            sqm_key = str(sqm)\n",
    "                            if sqm_key in new_inner:\n",
    "                                new_inner[sqm_key] = [\n",
    "                                    new_inner[sqm_key][0] + inner_v[0],\n",
    "                                    new_inner[sqm_key][1] + inner_v[1],\n",
    "                                ]\n",
    "                            else:\n",
    "                                new_inner[sqm_key] = inner_v\n",
    "                        else:\n",
    "                            if inner_k in new_inner:\n",
    "                                new_inner[inner_k] = [\n",
    "                                    new_inner[inner_k][0] + inner_v[0],\n",
    "                                    new_inner[inner_k][1] + inner_v[1],\n",
    "                                ]\n",
    "                            else:\n",
    "                                new_inner[inner_k] = inner_v\n",
    "                    new_dict[k] = new_inner\n",
    "                else:\n",
    "                    new_dict[k] = v\n",
    "            return new_dict\n",
    "        return obj\n",
    "\n",
    "    return str([convert_keys(d) for d in data])\n",
    "\n",
    "# Apply only where 'sqft' is mentioned\n",
    "def conditional_convert(row):\n",
    "    unit = str(row.get(unit_column, \"\")).strip().lower()\n",
    "    data = row.get(column_name, \"\")\n",
    "    if unit == \"sqft\":\n",
    "        return convert_sqft_to_sqmt(data)\n",
    "    else:\n",
    "        return data  # untouched\n",
    "\n",
    "# Apply conversion\n",
    "df[column_name + \"_converted\"] = df.apply(conditional_convert, axis=1)\n",
    "\n",
    "#Save result to file\n",
    "output_path = r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_sqft_converted.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\" Done! Duplicate sqm keys merged correctly.\\nConverted where 'sqft' specified.\\nSaved to:\\n{output_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b878cd5",
   "metadata": {},
   "source": [
    "# Sum_Problem_Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# # File path and sheet\n",
    "# file_path = r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_sqft_converted.xlsx\"\n",
    "\n",
    "# # Load Excel\n",
    "# df = pd.read_excel(file_path)\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Column names\n",
    "# column_name = \"Carpet_wise_total_sold_converted\"\n",
    "# unit_column = \"cwts(sum/avg)\"\n",
    "\n",
    "# # Function to divide area by count (only for 'sum' rows)\n",
    "def divide_area_by_count(data_str, is_3bhk=False):\n",
    "    try:\n",
    "        data = ast.literal_eval(data_str)\n",
    "    except:\n",
    "        return data_str  # return original if parsing fails\n",
    "\n",
    "    min_val = 105 if is_3bhk else 63\n",
    "\n",
    "    def transform_keys(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, dict):\n",
    "                    new_inner = {}\n",
    "                    for inner_k, inner_v in v.items():\n",
    "                        try:\n",
    "                            key_float = float(inner_k)\n",
    "                            if isinstance(inner_v, list) and inner_v[0] != 0 and key_float > min_val:\n",
    "                                divided = round(key_float / inner_v[0], 2)\n",
    "                                new_inner[str(divided)] = inner_v\n",
    "                            else:\n",
    "                                new_inner[inner_k] = inner_v\n",
    "                        except:\n",
    "                            new_inner[inner_k] = inner_v\n",
    "                    new_dict[k] = new_inner\n",
    "                else:\n",
    "                    new_dict[k] = v\n",
    "            return new_dict\n",
    "        return obj\n",
    "\n",
    "    return str([transform_keys(d) for d in data])\n",
    "\n",
    "# Apply only if 'cwts(sum/avg)' == 'sum'\n",
    "def conditional_divide(row):\n",
    "    unit = str(row.get(unit_column, \"\")).strip().lower()\n",
    "    data = row.get(column_name, \"\")\n",
    "    is_3bhk = \"3bhk\" in str(row).lower()\n",
    "    if unit == \"sum\":\n",
    "        return divide_area_by_count(data, is_3bhk)\n",
    "    else:\n",
    "        return data  # keep original if not 'sum'\n",
    "\n",
    "# Create new column with result\n",
    "df[column_name + \"_avg_converted\"] = df.apply(conditional_divide, axis=1)\n",
    "\n",
    "#Save to file\n",
    "output_path = r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_Sum_converted_demo.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\" Done! Values divided by count for 'sum' rows with min thresholds.\\nSaved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee756bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_Sum_converted_demo.xlsx\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0b4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4654404a",
   "metadata": {},
   "source": [
    "#do manual for mix and apartment number in CWTS & create \"Carpet_wise_total_sold_converted_manually\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f31fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Carpet_wise_total_sold_converted_avg_converted\"] = df[\"Carpet_wise_total_sold_converted_avg_converted\"].str.replace(\"..\", \".\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b795c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "# df = pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_Sum_converted_demo.xlsx\")\n",
    "\n",
    "# Function to safely parse stringified data\n",
    "def safe_literal_eval(x):\n",
    "    if pd.isna(x):  # Handle NaN or None\n",
    "        return {}  # Return an empty dict or adjust based on your needs\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}  # Return an empty dict for invalid strings or adjust as needed\n",
    "\n",
    "# Parse stringified lists/dicts\n",
    "df[\"Carpet_wise_total_sold_converted_avg_converted\"] = df[\"Carpet_wise_total_sold_converted_avg_converted\"].apply(safe_literal_eval)\n",
    "\n",
    "# Fix BHK_wise_CA structure based on Carpet_wise_total_sold_converted_manually\n",
    "def fix_bhk_wise_ca_format(row):\n",
    "    fixed = []\n",
    "    # Ensure row is a list or dict, handle empty/invalid cases\n",
    "    if not isinstance(row, (list, dict)):\n",
    "        return fixed\n",
    "    for carpet_dict in row:\n",
    "        new_entry = {}\n",
    "        for bhk_type, ca_dict in carpet_dict.items():\n",
    "            if isinstance(ca_dict, dict):\n",
    "                new_entry[bhk_type] = [str(k) for k in ca_dict.keys()]\n",
    "        fixed.append(new_entry)\n",
    "    return fixed\n",
    "\n",
    "# Apply the function to create the updated column\n",
    "df[\"BHK_wise_CA_Updated\"] = df[\"Carpet_wise_total_sold_converted_avg_converted\"].apply(fix_bhk_wise_ca_format)\n",
    "\n",
    "# Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70550fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\demo_1.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a64bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\demo_1.xlsx\")\n",
    "\n",
    "# Function to create Desire_output\n",
    "def generate_desire_output(carpet_data):\n",
    "    # Convert string to Python object if it's a string\n",
    "    if isinstance(carpet_data, str):\n",
    "        try:\n",
    "            carpet_data = ast.literal_eval(carpet_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []  # Return empty list if parsing fails\n",
    "\n",
    "    output = []\n",
    "    for item in carpet_data:\n",
    "        bhk_result = {}\n",
    "        for bhk, carpet_dict in item.items():\n",
    "            total_units = 0\n",
    "            sold_units = 0\n",
    "            area_keys = []\n",
    "\n",
    "            for area_str, counts in carpet_dict.items():\n",
    "                try:\n",
    "                    area = float(area_str.strip())\n",
    "                except:\n",
    "                    continue\n",
    "                total_units += counts[0]\n",
    "                sold_units += counts[1]\n",
    "                area_keys.append(area)\n",
    "\n",
    "            avg_area = sum(area_keys) / len(area_keys) if area_keys else 0\n",
    "            bhk_result[bhk.upper()] = {\n",
    "                \"total\": total_units,\n",
    "                \"sold\": sold_units,\n",
    "                \"avg_area\": avg_area\n",
    "            }\n",
    "\n",
    "        output.append(bhk_result)\n",
    "    return output\n",
    "\n",
    "# Apply transformation\n",
    "df[\"bhks_Updated\"] = df[\"Carpet_wise_total_sold_converted_avg_converted\"].apply(generate_desire_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\demo_2.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Demo_bhk_wise_CA_&_BHKs.xlsx\")\n",
    "\n",
    "\n",
    "# Calculate parityA\n",
    "def extract_totals(row):\n",
    "    # Parse bhks_Updated if it's a string\n",
    "    bhk_data = row[\"bhks_Updated\"]\n",
    "    if isinstance(bhk_data, str):\n",
    "        try:\n",
    "            bhk_data = ast.literal_eval(bhk_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            bhk_data = []  # Return empty list if parsing fails\n",
    "\n",
    "    bhk_total = bhk_sold = 0\n",
    "    for b in bhk_data:\n",
    "        for unit in b.values():\n",
    "            bhk_total += unit.get(\"total\", 0)\n",
    "            bhk_sold += unit.get(\"sold\", 0)\n",
    "\n",
    "    # Parse Carpet_wise_total_sold_converted_avg_converted if it's a string\n",
    "    carpet_data = row[\"Carpet_wise_total_sold_converted_avg_converted\"]\n",
    "    if isinstance(carpet_data, str):\n",
    "        try:\n",
    "            carpet_data = ast.literal_eval(carpet_data)\n",
    "        except (ValueError, SyntaxError):\n",
    "            carpet_data = []  # Return empty list if parsing fails\n",
    "\n",
    "    carpet_total = carpet_sold = 0\n",
    "    for b in carpet_data:\n",
    "        for flat in b.values():\n",
    "            for count in flat.values():\n",
    "                carpet_total += count[0]\n",
    "                carpet_sold += count[1]\n",
    "\n",
    "    bhk_count = len(bhk_data)\n",
    "    carpet_count = len(carpet_data)\n",
    "    return pd.Series([bhk_total, bhk_sold, bhk_count, carpet_total, carpet_sold, carpet_count])\n",
    "\n",
    "df[[\"bhk_total_after_manual\", \"bhk_sold_after_manual\", \"bhk_count_after_manual\", \n",
    "    \"carpet_total_after_manual\", \"carpet_sold_after_manual\", \"carpet_count_after_manual\"]] = df.apply(extract_totals, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef43af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If bhks and Building_name are stored as strings, convert them\n",
    "df[\"bhks_Updated\"] = df[\"bhks_Updated\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df[\"Building_name\"] = df[\"Building_name\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Now process the row to compute [[building_name, total, sold], ...]\n",
    "def get_building_units_list_format(row):\n",
    "    bhks = row[\"bhks_Updated\"]\n",
    "    names = row[\"Building_name\"]\n",
    "    result = []\n",
    "    for i in range(len(names)):\n",
    "        total = sold = 0\n",
    "        if i < len(bhks) and isinstance(bhks[i], dict):\n",
    "            for unit_data in bhks[i].values():\n",
    "                total += unit_data.get(\"total\", 0)\n",
    "                sold += unit_data.get(\"sold\", 0)\n",
    "        result.append([names[i], total, sold])\n",
    "    return result\n",
    "\n",
    "# Apply the function\n",
    "df[\"Building_wise_total_sold_units_after_manual\"] = df.apply(get_building_units_list_format, axis=1)\n",
    "# Compute total and sold units across all buildings in each list\n",
    "def compute_totals_from_list(building_list):\n",
    "    total_units = sum(item[1] for item in building_list)\n",
    "    sold_units = sum(item[2] for item in building_list)\n",
    "    return pd.Series([total_units, sold_units])\n",
    "\n",
    "df[[\"Total_units_sum_after_manual\", \"Sold_units_sum_after_manual\"]] = df[\"Building_wise_total_sold_units_after_manual\"].apply(compute_totals_from_list)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\demo_3.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parity(row):\n",
    "    return (\n",
    "        row[\"bhk_total_after_manual\"] == row[\"Total_units_sum_after_manual\"] and\n",
    "        row[\"carpet_total_after_manual\"] == row[\"Total_units_sum_after_manual\"] and\n",
    "        row[\"bhk_sold_after_manual\"] == row[\"Sold_units_sum_after_manual\"] and\n",
    "        row[\"carpet_sold_after_manual\"] == row[\"Sold_units_sum_after_manual\"]\n",
    "    )\n",
    "\n",
    "# Add parity column\n",
    "df[\"final_parity_after_manual\"] = df.apply(check_parity, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_parity_after_manual'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_Manually_Done.xlsx\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae241b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['final_parity_after_manual']==False]['Carpet_wise_total_sold_converted_refined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parity(row):\n",
    "    return (\n",
    "        row[\"Total_units_sum_after_manual\"] == row[\"carpet_total_after\"] and\n",
    "        row[\"Sold_units_sum_after_manual\"] == row[\"carpet_sold_after\"] \n",
    "    )\n",
    "\n",
    "# Add parity column\n",
    "df[\"final_parity_between_old_and_new\"] = df.apply(check_parity, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('01c_after_.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b7452",
   "metadata": {},
   "source": [
    "# Find out total supply and consume area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_carpet_size(value):\n",
    "    match = re.search(r'\\d+\\.\\d+|\\d+', value)\n",
    "    return float(match.group()) if match else 0.0\n",
    "\n",
    "def calculate_areas(cell):\n",
    "    total_supply = 0.0\n",
    "    total_consumed = 0.0\n",
    "    try:\n",
    "        parsed = ast.literal_eval(cell)\n",
    "        for unit in parsed:\n",
    "            for bhk_type, carpet_data in unit.items():\n",
    "                for carpet_size, counts in carpet_data.items():\n",
    "                    size_float = clean_carpet_size(carpet_size)\n",
    "                    supply_count = counts[0]\n",
    "                    consumed_count = counts[1]\n",
    "                    total_supply += size_float * supply_count\n",
    "                    total_consumed += size_float * consumed_count\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing cell: {e}\")\n",
    "    return total_supply, total_consumed\n",
    "\n",
    "\n",
    "# Apply function\n",
    "df[['total_supply_area', 'total_consumed_area']] = df['Carpet_wise_total_sold_converted_avg_converted'].apply(\n",
    "    lambda x: pd.Series(calculate_areas(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel(r\"D:\\Rera_Scraping_March_25\\Thane\\o1b_20241231_rera_final_review_Thane_Manually_Done.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rera_df = df[['Project Name', 'Project Status', 'Organization/Individual',\n",
    "       'developer_address', 'Proposed Date of Completion',\n",
    "       'Revised Proposed Date of Completion',\n",
    "       'Litigations related to the project ?', 'Project type', 'Plot no.',\n",
    "       'Building_name', 'Number of Sanctioned Floors', \"Number of Basement's\",\n",
    "       'Number of Plinth', \"Number of Podium's\", 'Number of Stilts',\n",
    "       'Total_no_of_open_Parking', 'Number_of_Closed_Parking',\n",
    "       'boundaries east', 'boundaries west', 'boundaries north',\n",
    "       'boundaries south', 'state', 'division', 'district', 'taluka',\n",
    "       'village', 'street', 'locality', 'pin code', 'Total Building Count',\n",
    "       'Sanctioned Buildings Count',\n",
    "       'Proposed But Not Sanctioned Buildings Count',\n",
    "       'Aggregate area(In sqmts) of recreational open space',\n",
    "       'Sactioned FSI of the Project Applied for Registration (Sanctioned Build-up Area)',\n",
    "       'Built-up-Area as per Proposed FSI (in sqfts)(Proposed but not Sanctioned)', 'Permissible Built-up Area',\n",
    "       'Bank_name', 'IFSC CODE', 'website of Organization', 'Contact',\n",
    "       'Past Experience', 'Past_experience_details',\n",
    "       'Total Plot/Project area (sqmts)',\n",
    "       'plot_details_in_No_of_Plot/Area_of_plot/Booked_plot',\n",
    "       'Building_Name', 'Rera_certificate_bottom_left_date',\n",
    "       'Rera_commmencing_from', 'Rera_commmencing_ending_with',\n",
    "       'Promoter Name', 'Last_Modified_Date', 'RERA ID',\n",
    "       'Building_wise_total_sold_units_after_manual',\n",
    "       'Carpet_wise_total_sold_converted_avg_converted',\n",
    "       'BHK_wise_CA_Updated',  'bhks_Updated','Total_units_sum_after_manual', 'Sold_units_sum_after_manual']]\n",
    "rera_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rera_df.rename(columns={\n",
    "    'Carpet_wise_total_sold_converted_refined':'Carpet_wise_total_sold',\n",
    "    'BHK_wise_CA_fixed':'BHK_wise_CA',\n",
    "    'bhks_Desire_output':'bhks',\n",
    "    'Total_units_sum_after_manual':'total_units',\n",
    "    'Sold_units_sum_after_manual':'total_sold_units',\n",
    "    'Building_wise_total_sold_units_after_manual':'Building_wise_total_sold'\n",
    "},inplace=True)\n",
    "rera_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rera_df['Project Name']=rera_df['Project Name'].str.strip().str.title()\n",
    "rera_df['Organization/Individual']=rera_df['Organization/Individual'].str.strip().str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_flatten(series):\n",
    "    result = []\n",
    "    for item in series:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(item)\n",
    "        elif isinstance(item, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(item)\n",
    "                if isinstance(parsed, list):\n",
    "                    result.extend(parsed)\n",
    "                else:\n",
    "                    result.append(parsed)\n",
    "            except:\n",
    "                result.append(item)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "aggregation_functions = {\n",
    "    'Project Name':'first', 'Project Status': \"first\", 'Organization/Individual':'first',\n",
    "    'developer_address':'first', 'Proposed Date of Completion': \"first\",\n",
    "    'Revised Proposed Date of Completion': \"first\", 'Litigations related to the project ?': \"first\",\n",
    "    'Project type': \"first\", 'Plot no.':\"first\",\n",
    "\n",
    "    'Building_name': safe_flatten,\n",
    "    'Number of Sanctioned Floors': safe_flatten,\n",
    "    \"Number of Basement's\": safe_flatten,\n",
    "    'Number of Plinth': safe_flatten,\n",
    "    \"Number of Podium's\": safe_flatten,\n",
    "    'Number of Stilts': safe_flatten,\n",
    "    'Total_no_of_open_Parking': safe_flatten,\n",
    "    'Number_of_Closed_Parking': safe_flatten,\n",
    "\n",
    "    'boundaries east':'first', 'boundaries west':'first', 'boundaries north':'first',\n",
    "    'boundaries south':'first', 'state':'first', 'division':'first', 'district':'first', 'taluka':'first',\n",
    "    'village':'first', 'street':'first', 'locality':'first', 'pin code':'first',\n",
    "\n",
    "    'Total Building Count':'sum',\n",
    "    'Sanctioned Buildings Count':'sum',\n",
    "    'Proposed But Not Sanctioned Buildings Count':'sum',\n",
    "    'Aggregate area(In sqmts) of recreational open space':'sum',\n",
    "    'Sactioned FSI of the Project Applied for Registration (Sanctioned Build-up Area)':'sum',\n",
    "    'Built-up-Area as per Proposed FSI (in sqfts)(Proposed but not Sanctioned)':'sum', 'Permissible Built-up Area':'sum',\n",
    "\n",
    "    'Bank_name':'first', 'IFSC CODE':'first', 'website of Organization':'first', 'Contact':'first',\n",
    "    'Past Experience':'first', 'Past_experience_details':'first',\n",
    "    'Total Plot/Project area (sqmts)':'sum',\n",
    "\n",
    "    'plot_details_in_No_of_Plot/Area_of_plot/Booked_plot': safe_flatten,\n",
    "    'Building_Name': safe_flatten,\n",
    "    'Building_wise_total_sold': safe_flatten,\n",
    "    'Carpet_wise_total_sold_converted_avg_converted': safe_flatten,\n",
    "    'BHK_wise_CA_Updated': safe_flatten,\n",
    "    'bhks_Updated': safe_flatten,\n",
    "    'RERA ID': safe_flatten,\n",
    "\n",
    "    'Rera_certificate_bottom_left_date':'first',\n",
    "    'Rera_commmencing_from':'first',\n",
    "    'Rera_commmencing_ending_with':'first',\n",
    "    'Promoter Name':'first',\n",
    "    'Last_Modified_Date':'first',\n",
    "    'total_units':'sum',\n",
    "    'total_sold_units':'sum'\n",
    "}\n",
    "\n",
    "df_rera_unique = (\n",
    "    rera_df.groupby(['Project Name', 'Organization/Individual'])\n",
    "    .aggregate(aggregation_functions)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_rera_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e93990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique['Building_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee377ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rera_unique['total_units'].sum())\n",
    "print(rera_df['total_units'].sum())\n",
    "print(df_rera_unique['total_sold_units'].sum())\n",
    "print(rera_df['total_sold_units'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e687ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique['rera_entry'] = df_rera_unique['RERA ID'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "df_rera_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037da759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique['rera_entry'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ee4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique.to_excel(r'D:\\Rera_Scraping_March_25\\Thane\\o1g_final_agreegated.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c431d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c115adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfd282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f2b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491dc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed45915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grand_excel=pd.read_excel(r\"D:\\RERA op\\rera main code\\Pune RERA GRAND EXCEL VERSION 9 Temp.xlsx\",sheet_name='Sheet1')\n",
    "df_grand_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13536c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Optional: create the columns if not already present\n",
    "df_rera_unique['Locationupdated'] = None\n",
    "df_rera_unique['Project_lat'] = None\n",
    "df_rera_unique['Project_lng'] = None\n",
    "\n",
    "# Iterate over grand excel rows\n",
    "for _, row in df_grand_excel.iterrows():\n",
    "    # Split multiline RERA Project field\n",
    "    projects = [p.strip() for p in str(row['RERA Project']).split(\"\\n\") if p.strip()]\n",
    "    \n",
    "    # Parse Organization list\n",
    "    try:\n",
    "        org_list = ast.literal_eval(row['Organization/Individual'])\n",
    "    except:\n",
    "        org_list = [row['Organization/Individual']]\n",
    "    \n",
    "    # Location and lat/lng values\n",
    "    location = row.get('Modified RERA Location - v3')\n",
    "    lat = row.get('Project_lat')\n",
    "    lng = row.get('Project_lng')\n",
    "\n",
    "    # Matching condition\n",
    "    condition = (df_rera_unique['Project Name'].isin(projects)) &                 (df_rera_unique['Organization/Individual'].isin(org_list))\n",
    "    \n",
    "    # Apply updates\n",
    "    df_rera_unique.loc[condition, 'Locationupdated'] = location\n",
    "    df_rera_unique.loc[condition, 'Project_lat'] = lat\n",
    "    df_rera_unique.loc[condition, 'Project_lng'] = lng\n",
    "df_rera_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate combinations in df_grand_excel\n",
    "dupes = df_grand_excel.groupby(['RERA Project', 'Organization/Individual']).size().reset_index(name='count')\n",
    "dupes = dupes[dupes['count'] > 1]\n",
    "\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27169152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique.to_excel('pune_final_rera_excel.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88ff71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel(r\"D:\\RERA op\\rera main code\\o1g_20250426_Pune_rera_with_location_manual.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.replace('\\xa0', ' ', regex=False)  # turn NBSP into normal space\n",
    "      .str.strip()                             # remove leading/trailing spaces\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Project Name']=df['Project Name'].str.strip().str.title()\n",
    "df['Location updated']=df['Location updated'].str.strip().str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9aa25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location updated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_flatten(series):\n",
    "    result = []\n",
    "    for item in series:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(item)\n",
    "        elif isinstance(item, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(item)\n",
    "                if isinstance(parsed, list):\n",
    "                    result.extend(parsed)\n",
    "                else:\n",
    "                    result.append(parsed)\n",
    "            except:\n",
    "                result.append(item)\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "aggregation_functions = {\n",
    "    'Project Name':'first', 'Project Status': \"first\", 'Organization/Individual':'first',\n",
    "    'developer_address':'first', 'Proposed Date of Completion': \"first\",\n",
    "    'revised date of completion': \"first\", 'Litigations related to the project ?': \"first\",\n",
    "    'Project type': \"first\", 'Plot no.':\"first\",\n",
    "\n",
    "    'Building_name': safe_flatten,\n",
    "    'Number of Sanctioned Floors': safe_flatten,\n",
    "    \"Number of Basement's\": safe_flatten,\n",
    "    'Number of Plinth': safe_flatten,\n",
    "    \"Number of Podium's\": safe_flatten,\n",
    "    'Number of Stilts': safe_flatten,\n",
    "    'Total_no_of_open_Parking': safe_flatten,\n",
    "    'Number_of_Closed_Parking': safe_flatten,\n",
    "\n",
    "    'boundaries east':'first', 'boundaries west':'first', 'boundaries north':'first',\n",
    "    'boundaries south':'first', 'state':'first', 'division':'first', 'district':'first', 'taluka':'first',\n",
    "    'village':'first', 'street':'first', 'locality':'first', 'pin code':'first',\n",
    "\n",
    "    'Total Building Count':'sum',\n",
    "    'Sanctioned Buildings Count':'sum',\n",
    "    'Proposed But Not Sanctioned Buildings Count':'sum',\n",
    "    'Aggregate area(In sqmts) of recreational open space':'sum',\n",
    "    'Built-up-Area as per Proposed FSI (In sqmts) ( Proposed but not sanctioned)':'sum',\n",
    "    'Built-up-Area as per Approved FSI (In sqmts)':'sum', 'TotalFSI':'sum',\n",
    "\n",
    "    'Bank_name':'first', 'IFSC CODE':'first', 'website of Organization':'first', 'Contact':'first',\n",
    "    'Past Experience':'first', 'Past_experience_details':'first',\n",
    "    'Total Plot/Project area (sqmts)':'sum',\n",
    "\n",
    "    'plot_details_in_No_of_Plot/Area_of_plot/Booked_plot': safe_flatten,\n",
    "    'Building_wise_completion_date': safe_flatten,\n",
    "    'Building_wise_total_sold': safe_flatten,\n",
    "    'Carpet_wise_total_sold': safe_flatten,\n",
    "    'BHK_wise_CA': safe_flatten,\n",
    "    'bhks': safe_flatten,\n",
    "    'RERA ID': safe_flatten,\n",
    "\n",
    "    'Commencement bottom left date':'first',\n",
    "    'Commencement Date':'first',\n",
    "    'Rera_commmencing_ending_with':'first',\n",
    "    'Promoter Name':'first',\n",
    "    'Last_Modified_Date':'first',\n",
    "    'total_units':'sum',\n",
    "    'total_sold_units':'sum',\n",
    "    'rera_entry':'sum',\n",
    "    'Project_lat':'first',\n",
    "    'Project_lng':'first',\n",
    "    'Location updated':'first'\n",
    "    \n",
    "}\n",
    "\n",
    "df_rera_unique = (\n",
    "    df.groupby(['Project Name', 'Location updated'])\n",
    "    .aggregate(aggregation_functions)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_rera_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rera_unique.to_excel(r\"D:\\RERA op\\rera main code\\o1h_20250426_Pune_rera_with_location_manual.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f028477",
   "metadata": {},
   "source": [
    "### Add RERA ID, LAT, and LNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6970a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://maharerait.mahaonline.gov.in/GIS/getData.ashx?GetMapData=Data\"\n",
    "print(url)\n",
    "# go to this url and copy all data\n",
    "# save copied data into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://maharerait.mahaonline.gov.in/GIS/getData.ashx?GetMapData=Data'\n",
    "\n",
    "response = requests.get(url, verify=False)\n",
    "data = response.json()\n",
    "\n",
    "# Save data to a file\n",
    "filename = r'o1d_20240811_THANE_rera_by-direct-link.json'  # Replace with your desired file name\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert json to xlsx\n",
    "jsn = pd.read_json(filename)\n",
    "jsn.to_csv(r\"D:\\RERA op\\rera main code\\o1e_20240811_THANE_rera_by-direct-link.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_direct_with_link = pd.read_csv(r\"D:\\RERA op\\rera main code\\o1e_20240811_THANE_rera_by-direct-link.csv\",index_col=[0])\n",
    "df_direct_with_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns\n",
    "df_filtered = df_direct_with_link.loc[:, df_direct_with_link.columns.intersection(['Name_of_Project', 'Project_District', 'Application_No', 'lat','lng'])]\n",
    "print(\"Shape\",df_filtered.shape)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data by district/City\n",
    "df_pune_coords = df_filtered[(df_filtered['Project_District'] == 'Pune')].reset_index(drop=True)\n",
    "df_pune_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ab508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make mumbai lat, lng data unique\n",
    "df_mumbai_coords_unique = (df_pune_coords.groupby(['Name_of_Project', 'Project_District', 'Application_No', 'lat', 'lng']).aggregate({'Name_of_Project':'first', 'Project_District':'first', 'Application_No':'first', 'lat':'first', 'lng':'first'})).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mumbai_coords_unique['Name_of_Project']=df_pune_coords['Name_of_Project'].apply(lambda x:x.title())\n",
    "df['Project Name']=df['Project Name'].apply(lambda x:x.title())\n",
    "df['locality']=df['locality'].apply(lambda x:str(x).title())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890151ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding latitude, longitude columns in df_rera_unique from df_mumbai_coords_unique\n",
    "df['rera_id'] = \"\"\n",
    "df['lat'] = \"\"\n",
    "df['lng'] = \"\"\n",
    "for i in range(len(df)):\n",
    "    pn1 = df['Project Name'][i]\n",
    "    loc1= df['locality'][i]\n",
    "    result1 = df_mumbai_coords_unique[df_mumbai_coords_unique['Name_of_Project'] == pn1]\n",
    "    if result1.shape[0] != 0:\n",
    "        rera_id = result1.iloc[0]['Application_No']\n",
    "        lat = result1.iloc[0]['lat']\n",
    "        lng = result1.iloc[0]['lng']\n",
    "\n",
    "    else:\n",
    "        rera_id = \"na\"\n",
    "        lat = \"na\"\n",
    "        lng = \"na\"\n",
    "\n",
    "    df['rera_id'][i] = rera_id\n",
    "    df['lat'][i] = lat\n",
    "    df['lng'][i] = lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r\"D:\\RERA op\\rera main code\\o1f_20240811_THANE_rera_with_rera_coords.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79293b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2ee87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73bfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc10b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
